{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aba3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'derivative_project05'\n",
    "import antropy as ant\n",
    "from antropy import lziv_complexity\n",
    "import matplotlib\n",
    "# matplotlib.use('Qt5Agg')\n",
    "# %matplotlib\n",
    "\n",
    "#%matplotlib qt #**\n",
    "\n",
    "#%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import plot, show\n",
    "import pathlib\n",
    "import mne\n",
    "import os\n",
    "import os.path as op\n",
    "from mne import viz\n",
    "from mne import io\n",
    "from mne.preprocessing import (ICA, create_eog_epochs, create_ecg_epochs, corrmap)\n",
    "import zipfile\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.utils import io\n",
    "import neurokit2 as nk\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import itertools\n",
    "from whatever import *\n",
    "import time\n",
    "\n",
    "from scipy.stats import zscore\n",
    "def standardize(lst):\n",
    "    mean = sum(lst) / len(lst)\n",
    "    variance = sum([((x - mean) ** 2) for x in lst]) / len(lst)\n",
    "    std_dev = variance ** 0.5\n",
    "    return [(x - mean) / std_dev for x in lst]\n",
    "\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.externals \n",
    "import joblib\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.model_selection import groupKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "    # from sklearn.metrics import accuracy_score\n",
    "    # from sklearn.metrics import confusion_matrix\n",
    "    # from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf7a418",
   "metadata": {},
   "source": [
    "# From ---> 'LZC TimeSeries Feature Extraction-updated (complete)'\n",
    "\n",
    "## 1. LZc calculation used code that was adapted from Schartner et al., 2015\n",
    "            - for each participant, and for each of thier segments, calculate normalized LZc\n",
    "            - for each participant, average the normalized LZc across all segments\n",
    "            - each participant has 1 value = subj-i(mean(norm(LZc(seg-i))))\n",
    "\n",
    "#### 1.5.-Prepoc & Feature Extraction\n",
    "            - Main \n",
    "            - dif brain regions (ROIs)\n",
    "                - TenSecEpochs_19el-1\n",
    "                - TenSecEpochs_19el-2\n",
    "                - TenSecEpochs_19el-3\n",
    "                - TenSecEpochs_19el-4\n",
    "                - TenSecEpochs_19el-5\n",
    "            - '10secEpochs'\n",
    "            - 'TenSecEpochs' : using schartner et al., 2015 code for LZc\n",
    "            - 'TenSecEpochs_woutreref'\n",
    "            - 'TenSecEpochs_max' : using from antropy import lziv_complexity\n",
    "            - 'TenSecEpochs_max_woutreref' \n",
    "            - 'TenSecEpochs_mat' : (preproc w/ filter before reref and highass 0.1) i already did this....\n",
    "            - 'TenSecEpochs_MatandMax' : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68399190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v4\n",
    "# Schartner et al., 2015\n",
    "from numpy import *\n",
    "from numpy.linalg import * \n",
    "from scipy import signal\n",
    "from scipy.signal import hilbert\n",
    "from scipy.stats import ranksums\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat\n",
    "from random import *\n",
    "from itertools import combinations\n",
    "from pylab import *\n",
    "# def Preproc(X):\n",
    "#     Xp=signal.detrend(X-mean(X), axis=0)\n",
    "#     return Xp\n",
    "def str_col(X):\n",
    "    co=len(X)\n",
    "    A=abs(hilbert(X))\n",
    "    TH=mean(A)\n",
    "    S='' \n",
    "    for t in range(co):\n",
    "        if A[t]>TH:\n",
    "            S+='1'\n",
    "        else:\n",
    "            S+='0'\n",
    "    return S\n",
    "def cpr(string):\n",
    "    d={} \n",
    "    w = ''\n",
    "    i=1\n",
    "    for c in string:\n",
    "        wc = w + c\n",
    "        if wc in d:\n",
    "            w = wc\n",
    "        else:\n",
    "            d[wc]=wc\n",
    "            w = c\n",
    "        i+=1\n",
    "    return len(d)\n",
    "\n",
    "def LZc(X):\n",
    "#     Xp = Preproc(X)\n",
    "    S = str_col(X) \n",
    "    Slist = list(S) \n",
    "    shuffle(Slist) \n",
    "    SS = '' \n",
    "    for l in range(len(Slist)): \n",
    "        SS += Slist[l]\n",
    "    return cpr(S)/float(cpr(SS))\n",
    "\n",
    "\n",
    "# Data\n",
    "fnames = sorted(os.listdir('./cutdata'))\n",
    "for f in fnames:\n",
    "    with io.capture_output() as captured:\n",
    "        raw = mne.io.read_raw_fif(f'./cutdata/{f}/{f}.fif', preload=True)\n",
    "        raw.info['bads'] = []\n",
    "        raw2 = raw.copy().pick_types(eeg = True)\n",
    "        raw3 = nk.mne_crop(raw2.copy(),smin=0, smax=75500, include_tmax=False)\n",
    "    with io.capture_output() as captured:\n",
    "        raw3.notch_filter(60)\n",
    "        raw3.filter(0.1, 50)\n",
    "        raw3.drop_channels(new_non_brain_el)\n",
    "        raw3.set_eeg_reference(ref_channels='average')\n",
    "    vars()[f] = raw3.copy()\n",
    "for window in ['10secwindows']:\n",
    "    print(window)\n",
    "    create_folder = f'./results/NewDataResults/save/{window}'\n",
    "    if not os.path.exists(create_folder):\n",
    "        os.makedirs(create_folder)\n",
    "        print (f'main folder created: {create_folder}')\n",
    "    else:\n",
    "        print(\"main save folder already exists.\")\n",
    "    # Complexity measure\n",
    "    m='LZc'\n",
    "    print ('\\n************************',m,'************************\\n')\n",
    "    measure = m\n",
    "    # create numpy save folder\n",
    "    create_folder2 = f'./results/NewDataResults/save/{window}/{m}'\n",
    "    if not os.path.exists(create_folder2):\n",
    "        os.makedirs(create_folder2)\n",
    "        print (f'secondary folder created: {create_folder2}')\n",
    "    else:\n",
    "        print(f\"secondary folder already exists ({m}).\")\n",
    "    svd = lambda x: LZc(x)\n",
    "    # main dataframe\n",
    "    vars()[m+window]= pd.DataFrame()\n",
    "    for f in fnames:\n",
    "        print ('_____',f, '_____')\n",
    "        data = vars()[f]\n",
    "        Elist = []\n",
    "        if window == '10secwindows':\n",
    "            # 10secwindows (30 columns)\n",
    "            for e in data.ch_names:\n",
    "                print (f'{f}_{e}', end=', ')\n",
    "                signal = data.get_data(picks = e)[0][500:]\n",
    "                signal_2d = signal.reshape(-1, 2500)\n",
    "                svd_2d = np.apply_along_axis(svd, axis=1, arr=signal_2d)\n",
    "                Elist.append(svd_2d)\n",
    "\n",
    "        vars()['df_'+f] = pd.DataFrame(Elist)\n",
    "        ID_with_suffix = [f+'_'+ID for ID in data.ch_names]\n",
    "        vars()['df_'+f].index = [ID_with_suffix]\n",
    "\n",
    "        vars()[m+window] = pd.concat([vars()[m+window],vars()['df_'+f]],axis=0) \n",
    "\n",
    "        vars()['df_'+f].to_csv(f'./results/NewDataResults/save/{window}/{m}/df_{f}.csv', index=True) \n",
    "        print (f'   - df_{f}.csv is saved')\n",
    "    vars()[m+window].to_csv(f'./results/NewDataResults/save/{window}/{m}/{m}_{window}.csv', index=True)\n",
    "    print (f'________________________________________________________\\n   - saved The Big GUy!!!!! ({m}_{window}.csv)\\n________________________________________________________\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f9be4",
   "metadata": {},
   "source": [
    "# NewDataResults\n",
    "- results\n",
    " - NewDataResults\n",
    "    - LZC\n",
    "        - TenSecEpochs\n",
    "            - dfLZC.csv\n",
    "            - dict_TenSecEpochs.json\n",
    "    - save\n",
    "        - 10secwindows\n",
    "            - svd1\n",
    "                - svd1_10secwindows.csv\n",
    "                - df_CCTC03_IT.csv\n",
    "                - ...\n",
    "            - svd2\n",
    "            - approx\n",
    "            - sample\n",
    "            - detrended\n",
    "            - katz\n",
    "            - cwpen\n",
    "            - bubben\n",
    "            - ll\n",
    "            - mswpen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65174097",
   "metadata": {},
   "source": [
    "# LZC\n",
    "- 'TenSecEpochs' : using schartner et al., 2015 code for LZc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef787b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2\n",
    "# Schartner et al., 2015\n",
    "from numpy import *\n",
    "from numpy.linalg import * \n",
    "from scipy import signal\n",
    "from scipy.signal import hilbert\n",
    "from scipy.stats import ranksums\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat\n",
    "from random import *\n",
    "from itertools import combinations\n",
    "from pylab import *\n",
    "\n",
    "# Detrend and normalize input data, X a multidimensional time series\n",
    "def Preproc(X):\n",
    "    ro,co=shape(X)\n",
    "    Xp = zeros((ro,co))\n",
    "    for i in range(ro):\n",
    "        Xp[i,:]=signal.detrend(X[i,:]-mean(X[i,:]), axis=0)\n",
    "    return Xp\n",
    "###################################################################\n",
    "### LZc - Lempel-Ziv Complexity, column-by-column concatenation ###\n",
    "###################################################################\n",
    "# Binarized input matrix concatenated comlumn-by-column = Sequience\n",
    "def str_col(Xp):\n",
    "    # Input: Continuous multidimensional time series\n",
    "    # Output: One string \n",
    "    ro,co=shape(Xp)\n",
    "    TH=zeros(ro)\n",
    "    A=zeros((ro,co))\n",
    "    for i in range(ro):#91 electrodes\n",
    "        A[i,:]=abs(hilbert(Xp[i,:])) #absolute value of (the Computed analytic signal, (using the Hilbert transform))\n",
    "        TH[i]=mean(A[i,:]) #Threshold\n",
    "    S='' # Sequence\n",
    "    for t in range(co): # for each timepoint\n",
    "        for i in range(ro): # for each electrode\n",
    "            if A[i,t]>TH[i]:\n",
    "                S+='1'\n",
    "            else:\n",
    "                S+='0'\n",
    "    return S\n",
    "\n",
    "# Lempel-Ziv-Welch compression of binary input string\n",
    "def cpr(string): #input S = sequence\n",
    "    # Lempel-Ziv-Welch compression of binary input string, e.g. string='0010101'. \n",
    "        #It outputs the size of the dictionary of binary words.\n",
    "    d={} \n",
    "    w = ''\n",
    "    i=1\n",
    "    for c in string: # string = sequence \n",
    "        wc = w + c\n",
    "        if wc in d:\n",
    "            w = wc\n",
    "        else:\n",
    "            d[wc]=wc\n",
    "            w = c\n",
    "        i+=1\n",
    "    return len(d)\n",
    "\n",
    "def LZc(X): # X a multidimensional time series\n",
    "    # Compute LZc and use shuffled result as normalization\n",
    "    Xp = Preproc(X) # Detrend and normalize input data, \n",
    "    S = str_col(Xp) # Binarized & concatenated comlumn-by-column = Sequience = S\n",
    "    Slist = list(S) # digits of string(S) --become-> elements in a list\n",
    "    shuffle(Slist) # shuffle in-place Slist=shuffled\n",
    "    SS = '' # SS = Shuffled-Sequience \n",
    "    for l in range(len(Slist)): \n",
    "        SS += Slist[l] # turning Shuffled_Slist back into a string \n",
    "    return cpr(S)/float(cpr(SS)) # Lempel-Ziv-Welch compression of binary input string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eac49d",
   "metadata": {},
   "source": [
    "## Preproc & Feature Extraction\n",
    "- Main \n",
    "- dif brain regions (ROIs)\n",
    "    - TenSecEpochs_19el-1\n",
    "    - TenSecEpochs_19el-2\n",
    "    - TenSecEpochs_19el-3\n",
    "    - TenSecEpochs_19el-4\n",
    "    - TenSecEpochs_19el-5\n",
    "\n",
    "\n",
    "- '10secEpochs'\n",
    "- 'TenSecEpochs' : using schartner et al., 2015 code for LZc\n",
    "- 'TenSecEpochs_woutreref'\n",
    "- 'TenSecEpochs_max' : using from antropy import lziv_complexity\n",
    "- 'TenSecEpochs_max_woutreref' \n",
    "- 'TenSecEpochs_mat' : (preproc w/ filter before reref and highass 0.1) i already did this....\n",
    "- 'TenSecEpochs_MatandMax' : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac4bce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = sorted(os.listdir('./cutdata'))\n",
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74d6a83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main save folder already exists.\n",
      "_____ CCTC03_IT _____ 0.9497973103672804\n",
      "_____ CCTC03_RS _____ 0.947136465131423\n",
      "_____ CCTC03_ST _____ 0.9375819746220097\n",
      "_____ CCTC03part2_IT _____ 0.9685464600543233\n",
      "_____ CCTC03part2_RS _____ 0.915221217870828\n",
      "_____ CCTC03part2_ST _____ 0.968019187452015\n",
      "_____ CCTC04_IT _____ 0.9672642768503988\n",
      "_____ CCTC04_RS _____ 0.9575073827474365\n",
      "_____ CCTC04_RSredo _____ 0.9483748478997612\n",
      "_____ CCTC04_ST _____ 0.9661086194534237\n",
      "_____ CCTC05_IT _____ 0.9363745939425333\n",
      "_____ CCTC05_RS _____ 0.9195532560389642\n",
      "_____ CCTC05_ST _____ 0.9264768960919513\n",
      "_____ CCTC07NETICUsedoff_IT _____ 0.9021788725698712\n",
      "_____ CCTC07NETICUsedoff_RS _____ 0.8896232852681621\n",
      "_____ CCTC07NETICUsedoff_ST _____ 0.8849666347293119\n",
      "_____ CCTC07NETICUsedon1_IT _____ 0.99053114867292\n",
      "_____ CCTC07NETICUsedon1_RS _____ 0.9932681972439955\n",
      "_____ CCTC07NETICUsedon1_ST _____ 0.9864950202003829\n",
      "_____ CCTC07_IT _____ 0.8762699384997243\n",
      "_____ CCTC07_RS _____ 0.8452693455288783\n",
      "_____ CCTC07_ST _____ 0.8628794959490723\n",
      "_____ HC01_IT _____ 0.9126484653152139\n",
      "_____ HC01_ST _____ 0.9101326370952676\n",
      "_____ HC02_IT _____ 0.9601398452154156\n",
      "_____ HC02_ST _____ 0.9628598233805967\n",
      "_____ HC03_IT _____ 0.8898227808090917\n",
      "_____ HC03_ST _____ 0.9452514764276768\n",
      "_____ HC04_IT _____ 0.8990314517981166\n",
      "_____ HC04_ST _____ 0.902390103673639\n",
      "_____ HC05_IT2nd _____ 0.8788685976097639\n",
      "_____ HC05_ST _____ 0.8692953644709533\n",
      "_____ HC06_IT _____ 0.8129387831793089\n",
      "_____ HC06_ST _____ 0.8620462856156774\n",
      "_____ HC07_IT _____ 0.7140586216371373\n",
      "_____ HC07_ST _____ 0.6659122871319266\n",
      "_____ HC08_IT _____ 0.9006761247655932\n",
      "_____ HC08_ST _____ 0.8885137576358579\n",
      "_____ HC09_IT _____ 0.9022957784405072\n",
      "_____ HC09_ST _____ 0.8905627088568732\n",
      "_____ HC10_IT _____ 0.9358961695652401\n",
      "_____ HC10_ST _____ 0.9402513140636479\n",
      "_____ HC11_IT _____ 0.9071559828539502\n",
      "_____ HC11_ST _____ 0.9026166843104336\n",
      "_____ HC12_IT _____ 0.9186725907278418\n",
      "_____ HC12_ST _____ 0.8966441912308815\n",
      "_____ HC13_IT _____ 0.9131309261136731\n",
      "_____ HC13_ST _____ 0.9004270058399367\n",
      "_____ HC14_IT _____ 0.9409977638709521\n",
      "_____ HC14_ST _____ 0.9312586815217754\n",
      "_____ HC15_IT _____ 0.9371975155973988\n",
      "_____ HC15_ST _____ 0.9242992342952122\n",
      "_____ HC16_IT _____ 0.9075006546011355\n",
      "_____ HC16_ST _____ 0.9051718372398182\n",
      "_____ HC17_IT _____ 0.9255214561860365\n",
      "_____ HC17_ST _____ 0.9245416876220334\n",
      "_____ HC18_IT _____ 0.8857679146073812\n",
      "_____ HC18_ST _____ 0.863892616765745\n",
      "_____ MSICU05_IT _____ 0.8695930170385281\n",
      "_____ MSICU05_RS _____ 0.8594534192181195\n",
      "_____ MSICU05_ST _____ 0.8772305942804893\n",
      "_____ MSICU05part2_IT _____ 0.8979078255767192\n",
      "_____ MSICU05part2_RS _____ 0.8743864277061465\n",
      "_____ MSICU05part2_ST _____ 0.8823246601787041\n",
      "_____ MSICU07_IT _____ 0.8603926385419964\n",
      "_____ MSICU07_RS _____ 0.8529514840337515\n",
      "_____ MSICU07_ST _____ 0.8516985783295857\n",
      "_____ MSICU09_IT _____ 0.9342799100778246\n",
      "_____ MSICU09_RS _____ 0.8936820420771018\n",
      "_____ MSICU09_ST _____ 0.9268110464532813\n",
      "_____ MSICU09part2_IT _____ 0.89751691721404\n",
      "_____ MSICU09part2_RS _____ 0.8397727250835534\n",
      "_____ MSICU09part2_ST _____ 0.8639033975594238\n",
      "_____ MSICU10_IT _____ 0.950312451084638\n",
      "_____ MSICU10_RS _____ 0.9291297864277152\n",
      "_____ MSICU10_ST _____ 0.9533062475060096\n",
      "_____ MSICU14_IT _____ 0.9200406996804207\n",
      "_____ MSICU14_RS _____ 0.9205062105588521\n",
      "_____ MSICU14_ST _____ 0.8867248642862157\n",
      "_____ MSICU15_IT _____ 0.9064649255348953\n",
      "_____ MSICU15_RS _____ 0.8772841032896517\n",
      "_____ MSICU15_ST _____ 0.8951275356358042\n",
      "_____ MSICU16_IT _____ 0.737286162842988\n",
      "_____ MSICU16_RS _____ 0.7132068503732841\n",
      "_____ MSICU16_ST _____ 0.729299199859989\n",
      "_____ MSICU18_IT _____ 0.8681668868614693\n",
      "_____ MSICU18_RS _____ 0.8594587916512081\n",
      "_____ MSICU18_ST _____ 0.8776137489074054\n",
      "_____ MSICU19_IT _____ 0.9499793380555956\n",
      "_____ MSICU19_RS _____ 0.9178395951734383\n",
      "_____ MSICU19_ST _____ 0.9396130803691698\n",
      "_____ MSICU21_IT _____ 0.9328280169897458\n",
      "_____ MSICU21_RS _____ 0.9156054321140306\n",
      "_____ MSICU21_ST _____ 0.9339006802317075\n",
      "_____ MSICU22_IT _____ 0.9356148892849689\n",
      "_____ MSICU22_RS _____ 0.9297094885505889\n",
      "_____ MSICU22_ST _____ 0.9506895962088716\n",
      "_____ MSICU26_IT _____ 0.7592468654777507\n",
      "_____ MSICU26_RS _____ 0.8416786398837278\n",
      "_____ MSICU26_ST _____ 0.7811196016715214\n",
      "_____ MSICU28_IT _____ 0.9780949249315306\n",
      "_____ MSICU28_RS _____ 0.9790552506189965\n",
      "_____ MSICU28_ST _____ 0.981007199607882\n",
      "_____ MSICU29_IT _____ 0.8622383184858049\n",
      "_____ MSICU29_RS _____ 0.8225468271681874\n",
      "_____ MSICU29_ST _____ 0.83816997251208\n",
      "_____ MSICU30_IT _____ 0.902617047913004\n",
      "_____ MSICU30_RS _____ 0.9100632942860596\n",
      "_____ MSICU30_ST _____ 0.9156931704170463\n",
      "_____ MSICU32_IT _____ 0.8086950415571245\n",
      "_____ MSICU32_RS _____ 0.8932862693882725\n",
      "_____ MSICU32_ST _____ 0.8023441150543081\n",
      "_____ MSICU38_IT _____ 0.9132353230401018\n",
      "_____ MSICU38_RS _____ 0.8725595500874918\n",
      "_____ MSICU38_ST _____ 0.9458179554629169\n",
      "_____ MSICU42_IT _____ 0.9138203390985069\n",
      "_____ MSICU42_RS _____ 0.9265946835814289\n",
      "_____ MSICU42_ST _____ 0.8864709048506294\n",
      "_____ MSICU43_IT _____ 0.9712038467423701\n",
      "_____ MSICU43_RS _____ 0.9600137003021205\n",
      "_____ MSICU43_ST _____ 0.9703532180051607\n",
      "_____ MSICU47_IT _____ 0.9480250626561152\n",
      "_____ MSICU47_RS _____ 0.9411595547856513\n",
      "_____ MSICU47_ST _____ 0.9516391559386607\n",
      "_____ NETICU01sedoff_IT _____ 0.7965192285307383\n",
      "_____ NETICU01sedoff_RS _____ 0.8083908929892707\n",
      "_____ NETICU01sedoff_ST _____ 0.7722027068967553\n",
      "_____ NETICU01sedon1_IT _____ 0.7392238503822448\n",
      "_____ NETICU01sedon1_RS _____ 0.7877203935811685\n",
      "_____ NETICU01sedon1_ST _____ 0.7345161044640697\n",
      "_____ NETICU02sedoff_IT _____ 0.8188310101867765\n",
      "_____ NETICU02sedoff_RS _____ 0.8456821153194396\n",
      "_____ NETICU02sedoff_ST _____ 0.8418517552092106\n",
      "_____ NETICU02sedon1_IT _____ 0.6222477121576274\n",
      "_____ NETICU02sedon1_RS _____ 0.5475418887905887\n",
      "_____ NETICU02sedon1_ST _____ 0.615873514633106\n",
      "_____ NETICU03sedoff_IT _____ 0.8667650313298226\n",
      "_____ NETICU03sedoff_RS _____ 0.8492955240869823\n",
      "_____ NETICU03sedoff_ST _____ 0.8794933200991017\n",
      "_____ NETICU03sedon1_IT _____ 0.7516816163979957\n",
      "_____ NETICU03sedon1_RS _____ 0.6626963548988581\n",
      "_____ NETICU03sedon1_ST _____ 0.7335258742644447\n",
      "_____ NETICU04sedoff_IT _____ 0.7673242818493156\n",
      "_____ NETICU04sedoff_RS _____ 0.8969322589130706\n",
      "_____ NETICU04sedoff_ST _____ 0.7782978201277966\n",
      "_____ NETICU04sedon1_IT _____ 0.7271971715560426\n",
      "_____ NETICU04sedon1_RS _____ 0.8808040023533766\n",
      "_____ NETICU04sedon1_ST _____ 0.715040893041059\n",
      "_____ NETICU05sedoff_IT _____ 0.9552070385309387\n",
      "_____ NETICU05sedoff_RS _____ 0.933242743821979\n",
      "_____ NETICU05sedoff_ST _____ 0.9522049233116002\n",
      "_____ NETICU05sedon1_IT _____ 0.9346097176723596\n",
      "_____ NETICU05sedon1_RS _____ 0.9134524424361911\n",
      "_____ NETICU05sedon1_ST _____ 0.8690071145238304\n",
      "_____ NETICU06sedoff_IT _____ 0.808658994823093\n",
      "_____ NETICU06sedoff_RS _____ 0.925397341693369\n",
      "_____ NETICU06sedoff_ST _____ 0.8048547879639023\n",
      "_____ NETICU06sedon1_IT _____ 0.7747380767039539\n",
      "_____ NETICU06sedon1_RS _____ 0.9276865395626894\n",
      "_____ NETICU06sedon1_ST _____ 0.7737004464874558\n",
      "_____ NETICU08sedoff_IT _____ 0.8372750175364098\n",
      "_____ NETICU08sedoff_ST _____ 0.753487680260653\n",
      "_____ NETICU08sedon2_IT _____ 0.9053534409741503\n",
      "_____ NETICU08sedon2_RS _____ 0.9049843874597733\n",
      "_____ NETICU08sedon2_ST _____ 0.909662306791726\n",
      "_____ NETICU11sedoff_IT _____ 0.9503243871636464\n",
      "_____ NETICU11sedoff_RS _____ 0.9685657192814152\n",
      "_____ NETICU11sedoff_ST _____ 0.9487824481717309\n",
      "_____ NETICU11sedon1_IT _____ 0.95144246530759\n",
      "_____ NETICU11sedon1_RS _____ 0.9594633039801378\n",
      "_____ NETICU11sedon1_ST _____ 0.9384077093709571\n",
      "_____ NETICU13sedoff_IT _____ 0.9277949103770784\n",
      "_____ NETICU13sedoff_RS _____ 0.8555426009827858\n",
      "_____ NETICU13sedoff_ST _____ 0.916466380095275\n",
      "_____ NETICU13sedon1_IT _____ 0.7258619813619692\n",
      "_____ NETICU13sedon1_RS _____ 0.7224635062268283\n",
      "_____ NETICU13sedon1_ST _____ 0.718310373383094\n",
      "_____ NETICU15sedoff_IT _____ 0.8808795009833745\n",
      "_____ NETICU15sedoff_RS _____ 0.8691178160844505\n",
      "_____ NETICU15sedoff_ST _____ 0.8641792230292517\n",
      "_____ NETICU15sedon1_IT _____ 0.7231658160125404\n",
      "_____ NETICU15sedon1_RS _____ 0.8383700131604233\n",
      "_____ NETICU15sedon1_ST _____ 0.7102003310311414\n",
      "_____ NETICU17sedoff_IT _____ 0.9285195583709559\n",
      "_____ NETICU17sedoff_RS _____ 0.9006263972896517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____ NETICU17sedoff_ST _____ 0.9285722493928384\n",
      "_____ NETICU17sedon1_IT _____ 0.8999569006154172\n",
      "_____ NETICU17sedon1_RS _____ 0.9015026685562592\n",
      "_____ NETICU17sedon1_ST _____ 0.8910938823792716\n",
      "_____ NETICU18sedoff_RS _____ 0.9308529224069301\n",
      "_____ NETICU18sedon1_IT _____ 0.8769721742974528\n",
      "_____ NETICU18sedon1_RS _____ 0.944238494208667\n",
      "_____ NETICU18sedon1_ST _____ 0.908562626159079\n",
      "_____ NETICU19sedoff_IT _____ 0.9256386978752017\n",
      "_____ NETICU19sedoff_RS _____ 0.9498911163124961\n",
      "_____ NETICU19sedoff_ST _____ 0.8779291528504231\n",
      "_____ NETICU19sedon1_IT _____ 0.9602617921447701\n",
      "_____ NETICU19sedon1_RS _____ 0.9643715944335645\n",
      "_____ NETICU19sedon1_ST _____ 0.9570453741221581\n",
      "_____ NETICU20sedoff_RS _____ 0.8126691573493527\n",
      "_____ NETICU20sedon1_IT _____ 0.6910562583579527\n",
      "_____ NETICU20sedon1_RS _____ 0.7222146212436569\n",
      "_____ NETICU20sedon1_ST _____ 0.7888470823852392\n",
      "_____ NETICU22sedoff_IT _____ 0.8474818034004381\n",
      "_____ NETICU22sedoff_RS _____ 0.8463395562866137\n",
      "_____ NETICU22sedoff_ST _____ 0.8540703518032805\n",
      "_____ NETICU22sedon1_IT _____ 0.8974368858222694\n",
      "_____ NETICU22sedon1_RS _____ 0.9182632496571042\n",
      "_____ NETICU22sedon1_ST _____ 0.9108477405396697\n",
      "_____ NETICU24sedoff_IT _____ 0.9478390196628701\n",
      "_____ NETICU24sedoff_RS _____ 0.9515424830835111\n",
      "_____ NETICU24sedoff_ST _____ 0.9494942737363673\n",
      "_____ NETICU24sedon1_IT _____ 0.7801279074432443\n",
      "_____ NETICU24sedon1_RS _____ 0.9622750448410528\n",
      "_____ NETICU24sedon1_ST _____ 0.8236184851536379\n",
      "_____ NETICU25sedoff_RS _____ 0.8831799728525632\n",
      "_____ NETICU25sedon1_IT _____ 0.825635661180337\n",
      "_____ NETICU25sedon1_RS _____ 0.8094534286868034\n",
      "_____ NETICU25sedon1_ST _____ 0.8317899383329408\n",
      "_____ NETICU27sedoff_IT _____ 0.9226447124301039\n",
      "_____ NETICU27sedoff_RS _____ 0.9357918983885095\n",
      "_____ NETICU27sedoff_ST _____ 0.9187563794513148\n",
      "_____ NETICU27sedon1_IT _____ 0.937575126956925\n",
      "_____ NETICU27sedon1_RS _____ 0.9303803005722543\n",
      "_____ NETICU27sedon1_ST _____ 0.9349062575882849\n",
      "_____ NETICU28sedoff_IT _____ 0.8932629573343739\n",
      "_____ NETICU28sedoff_RS _____ 0.8807642541279732\n",
      "_____ NETICU28sedoff_ST _____ 0.8943186040401073\n",
      "_____ NETICU28sedon1_IT _____ 0.9289160650706064\n",
      "_____ NETICU28sedon1_RS _____ 0.9414430659447798\n",
      "_____ NETICU28sedon1_ST _____ 0.9101365404116111\n",
      "_____ NETICU29sedoff_IT _____ 0.8936998849160976\n",
      "_____ NETICU29sedoff_RS _____ 0.9006207786294234\n",
      "_____ NETICU29sedoff_ST _____ 0.903361896326043\n",
      "_____ NETICU29sedon1_IT _____ 0.9270918530039579\n",
      "_____ NETICU29sedon1_RS _____ 0.9460746861287184\n",
      "_____ NETICU29sedon1_ST _____ 0.9223491315463194\n",
      "_____ NETICU30sedoff_IT _____ 0.8477782247270489\n",
      "_____ NETICU30sedoff_RS _____ 0.8449197460548521\n",
      "_____ NETICU30sedoff_ST _____ 0.8431241744907237\n",
      "_____ NETICU30sedon1_IT _____ 0.7619933940200984\n",
      "_____ NETICU30sedon1_RS _____ 0.8699324844984071\n",
      "_____ NETICU30sedon1_ST _____ 0.8246808706649718\n",
      "_____ NETICU31sedoff_IT _____ 0.8838615553951182\n",
      "_____ NETICU31sedoff_RS _____ 0.9172542396507654\n",
      "_____ NETICU31sedoff_ST _____ 0.8658760538482785\n",
      "_____ NETICU31sedon1_IT _____ 0.8996324333233586\n",
      "_____ NETICU31sedon1_ST _____ 0.8965910144864487\n",
      "_____ NETICU31sedon2_RS _____ 0.9277109749073233\n",
      "_____ NETICU33sedoff_RS _____ 0.8907594787056791\n",
      "_____ NETICU33sedon1_IT _____ 0.8639480630092456\n",
      "_____ NETICU33sedon1_RS _____ 0.8998998736289804\n",
      "_____ NETICU33sedon1_ST _____ 0.84895604514086\n",
      "_____ NETICU38sedon1_IT _____ 0.9427178332132765\n",
      "_____ NETICU38sedon1_RS _____ 0.9594180815570684\n",
      "_____ NETICU38sedon1_ST _____ 0.9399355754318325\n",
      "_____ NETICU40sedoff_IT _____ 0.8952959315752956\n",
      "_____ NETICU40sedoff_RS _____ 0.8998269413377323\n",
      "_____ NETICU40sedoff_ST _____ 0.9054459473352069\n",
      "_____ NETICU40sedon1_IT _____ 0.9766527272943571\n",
      "_____ NETICU40sedon1_RS _____ 0.9819328272885449\n",
      "_____ NETICU40sedon1_ST _____ 0.9705009266065023\n",
      "_____ NETICU41sedoff_IT _____ 0.9279466163143033\n",
      "_____ NETICU41sedoff_RS _____ 0.9239187292831598\n",
      "_____ NETICU41sedoff_ST _____ 0.9247913913385096\n",
      "_____ NETICU41sedon1_IT _____ 0.8897160662215096\n",
      "_____ NETICU41sedon1_RS _____ 0.8838569764825379\n",
      "_____ NETICU41sedon1_ST _____ 0.888023741640924\n",
      "_____ NETICU41sedon2_IT _____ 0.9109226729390273\n",
      "_____ NETICU41sedon2_RS _____ 0.8943356167839219\n",
      "_____ NETICU41sedon2_ST _____ 0.9012816639344995\n",
      "_____ NETICU46sedoff_RS _____ 0.8792398101705589\n",
      "_____ NETICU46sedon1_IT _____ 0.8777502702596688\n",
      "_____ NETICU46sedon1_RS _____ 0.8790707635353728\n",
      "_____ NETICU46sedon1_ST _____ 0.8875446158676925\n",
      "   - dfLZC.csv is saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CCTC03_IT</th>\n",
       "      <td>0.949797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCTC03_RS</th>\n",
       "      <td>0.947136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCTC03_ST</th>\n",
       "      <td>0.937582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCTC03part2_IT</th>\n",
       "      <td>0.968546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCTC03part2_RS</th>\n",
       "      <td>0.915221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NETICU41sedon2_ST</th>\n",
       "      <td>0.901282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NETICU46sedoff_RS</th>\n",
       "      <td>0.879240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NETICU46sedon1_IT</th>\n",
       "      <td>0.877750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NETICU46sedon1_RS</th>\n",
       "      <td>0.879071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NETICU46sedon1_ST</th>\n",
       "      <td>0.887545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "CCTC03_IT          0.949797\n",
       "CCTC03_RS          0.947136\n",
       "CCTC03_ST          0.937582\n",
       "CCTC03part2_IT     0.968546\n",
       "CCTC03part2_RS     0.915221\n",
       "...                     ...\n",
       "NETICU41sedon2_ST  0.901282\n",
       "NETICU46sedoff_RS  0.879240\n",
       "NETICU46sedon1_IT  0.877750\n",
       "NETICU46sedon1_RS  0.879071\n",
       "NETICU46sedon1_ST  0.887545\n",
       "\n",
       "[275 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NEW DATA \n",
    "#MAIN                                                #(Complete)\n",
    "window = '0dot08SecEpochs' #'TenSecEpochs' #'TenSecEpochs_max_woutreref'#'TenSecEpochs_mat'#'TenSecEpochs_MatandMax' #'TenSecEpochs_woutreref' # window = '10secEpochs'\n",
    "# create save folders\n",
    "create_folder = f'./results/NewDataResults/LZC/{window}'\n",
    "if not os.path.exists(create_folder):\n",
    "    os.makedirs(create_folder)\n",
    "    print (f'main folder created: {create_folder}')\n",
    "else:\n",
    "    print(\"main save folder already exists.\")\n",
    "\n",
    "## 10secEpochs / Broadband\n",
    "fnames = sorted(os.listdir('./cutdata'))\n",
    "ssfnames = [f for f in fnames if 'HC05_IT1st' not in f]\n",
    "for f in ssfnames:\n",
    "    with io.capture_output() as captured:# Load data\n",
    "        raw = mne.io.read_raw_fif(f'./cutdata/{f}/{f}.fif', preload=True)\n",
    "        # raw = mne.io.read_raw_fif(f'./cutdata/CCTC03_RS/CCTC03_RS.fif', preload=True)\n",
    "        raw.info['bads']= []\n",
    "        raw2 = raw.copy().pick_types(eeg = True)\n",
    "        raw3 = nk.mne_crop(raw2.copy(),smin=0, smax=75000, include_tmax=False)\n",
    "    with io.capture_output() as captured:# Preprocessing\n",
    "        raw3.notch_filter(60)# Filter\n",
    "        raw3.filter(0.1, 50) #downsample after filtering??????\n",
    "        raw3.drop_channels(new_non_brain_el)# Remove non-brain channels\n",
    "        \n",
    "        \n",
    "        \n",
    "        raw3.set_eeg_reference(ref_channels='average')# Re-refrence\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #vars()['raw_'+f] = raw3.copy() # Saving variable names \n",
    "        #data = raw3.copy()# Epoch\n",
    "        epochs = mne.make_fixed_length_epochs(raw3.copy(), duration=0.08, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs=epochs.get_data() #(30, 91, 2500)... (15000, 91, 5)\n",
    "    vars()[f] = Aepochs\n",
    "        \n",
    "# main dataframe\n",
    "subResults = [] # Results from each subjects AverageLZc (average across participant's segments)\n",
    "dict_LZcSegmentsList = {} # LZc values for each segment (NOT averaged)\n",
    "\n",
    "for f in ssfnames:\n",
    "    data = vars()[f]\n",
    "    segments, channels, timepoints = shape(data) #(30, 91, 2500)\n",
    "    LZcSegmentsList = []\n",
    "    for i in range(segments):\n",
    "        X = data[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList.append(LZcSegment)\n",
    "        #print (f'{i} --- {LZcSegment}')\n",
    "    AverageLZc = sum(LZcSegmentsList)/len(LZcSegmentsList) #mean(LZcSegmentsList)\n",
    "    print ('_____',f, '_____', AverageLZc)\n",
    "    vars()['AverageLZC_'+f] = AverageLZc\n",
    "    subResults.append(AverageLZc)\n",
    "    vars()['LZcSegmentsList_'+f] = LZcSegmentsList\n",
    "    dict_LZcSegmentsList[f] = LZcSegmentsList\n",
    "\n",
    "dfLZC = pd.DataFrame(subResults, ssfnames)#, ['LZC_values'])\n",
    "# dfLZC = pd.DataFrame(subResults)\n",
    "# dfLZC.index = [ssfnames]\n",
    "\n",
    "\n",
    "# save\n",
    "dfLZC.to_csv(f'./results/NewDataResults/LZC/{window}/dfLZC.csv', index=True)  \n",
    "print ('   - dfLZC.csv is saved')\n",
    "display(dfLZC)\n",
    "# save/write dict\n",
    "import json\n",
    "with open(f'./results/NewDataResults/LZC/{window}/dict_{window}.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList, file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW DATA \n",
    "#MAIN                                                #(Complete)\n",
    "#window = 'TenSecEpochs_max' #'TenSecEpochs' #'TenSecEpochs_max_woutreref'#'TenSecEpochs_mat'#'TenSecEpochs_MatandMax' #'TenSecEpochs_woutreref' # window = '10secEpochs'\n",
    "#create save folders\n",
    "for  window in ['0dot2SecEpochs','1SecEpochs','5SecEpochs']:\n",
    "    create_folder = f'./results/NewDataResults/LZC/{window}'\n",
    "    if not os.path.exists(create_folder):\n",
    "        os.makedirs(create_folder)\n",
    "        print (f'main folder created: {create_folder}')\n",
    "    else:\n",
    "        print(\"main save folder already exists.\")\n",
    "## 10secEpochs / Broadband\n",
    "fnames = sorted(os.listdir('./cutdata'))\n",
    "ssfnames = [f for f in fnames if 'HC05_IT1st' not in f]\n",
    "for f in ssfnames:\n",
    "    with io.capture_output() as captured:# Load data\n",
    "        raw = mne.io.read_raw_fif(f'./cutdata/{f}/{f}.fif', preload=True)\n",
    "        # raw = mne.io.read_raw_fif(f'./cutdata/CCTC03_RS/CCTC03_RS.fif', preload=True)\n",
    "        raw.info['bads']= []\n",
    "        raw2 = raw.copy().pick_types(eeg = True)\n",
    "        raw3 = nk.mne_crop(raw2.copy(),smin=0, smax=75000, include_tmax=False)\n",
    "    with io.capture_output() as captured:# Preprocessing\n",
    "        raw3.notch_filter(60)# Filter\n",
    "        raw3.filter(0.1, 50) #downsample after filtering??????\n",
    "        raw3.drop_channels(new_non_brain_el)# Remove non-brain channels\n",
    "        \n",
    "        \n",
    "        \n",
    "        raw3.set_eeg_reference(ref_channels='average')# Re-refrence\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #vars()['raw_'+f] = raw3.copy() # Saving variable names \n",
    "        #data = raw3.copy()# Epoch\n",
    "        epochs1 = mne.make_fixed_length_epochs(raw3.copy(), duration=0.2, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs1=epochs1.get_data() #(30, 91, 2500)\n",
    "        epochs2 = mne.make_fixed_length_epochs(raw3.copy(), duration=1, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs2=epochs2.get_data() #(30, 91, 2500)\n",
    "        epochs3 = mne.make_fixed_length_epochs(raw3.copy(), duration=5, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs3=epochs3.get_data() #(30, 91, 2500)\n",
    "    vars()[f] = Aepochs1\n",
    "    vars()[f+'_1SecEpochs'] = Aepochs2\n",
    "    vars()[f+'_5SecEpochs'] = Aepochs3\n",
    "    \n",
    "    \n",
    "    \n",
    "# main dataframe\n",
    "subResults1 = [] # Results from each subjects AverageLZc (average across participant's segments)\n",
    "subResults2 = []\n",
    "subResults3 = []\n",
    "dict_LZcSegmentsList1 = {} # LZc values for each segment (NOT averaged)\n",
    "dict_LZcSegmentsList2 = {}\n",
    "dict_LZcSegmentsList3 = {} \n",
    "#for  window in ['0dot2SecEpochs','1SecEpochs','5SecEpochs']:\n",
    "for f in ssfnames:\n",
    "    data1 = vars()[f]\n",
    "    data2 = vars()[f+'_1SecEpochs']\n",
    "    data3 = vars()[f+'_5SecEpochs']\n",
    "    segments1, channels1, timepoints1 = shape(data1) #(30, 91, 2500)\n",
    "    segments2, channels2, timepoints2 = shape(data2) #(30, 91, 2500)\n",
    "    segments3, channels3, timepoints3 = shape(data3) #(30, 91, 2500)\n",
    "    LZcSegmentsList1 = []\n",
    "    LZcSegmentsList2 = []\n",
    "    LZcSegmentsList3 = []\n",
    "    for i in range(segments1):\n",
    "        X = data1[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList1.append(LZcSegment)\n",
    "        #print (f'{i} --- {LZcSegment}')\n",
    "    for i in range(segments2):\n",
    "        X = data2[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList2.append(LZcSegment)\n",
    "    for i in range(segments3):\n",
    "        X = data3[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList3.append(LZcSegment)\n",
    "    AverageLZc1 = sum(LZcSegmentsList1)/len(LZcSegmentsList1) #mean(LZcSegmentsList)\n",
    "    AverageLZc2 = sum(LZcSegmentsList2)/len(LZcSegmentsList2) #mean(LZcSegmentsList)\n",
    "    AverageLZc3 = sum(LZcSegmentsList3)/len(LZcSegmentsList3) #mean(LZcSegmentsList)\n",
    "    print ('_____',f, '_____', AverageLZc1)\n",
    "    vars()['AverageLZC_'+f] = AverageLZc1\n",
    "    subResults1.append(AverageLZc1)\n",
    "    subResults2.append(AverageLZc2)\n",
    "    subResults3.append(AverageLZc3)\n",
    "    dict_LZcSegmentsList1[f] = LZcSegmentsList1\n",
    "    dict_LZcSegmentsList2[f] = LZcSegmentsList2\n",
    "    dict_LZcSegmentsList3[f] = LZcSegmentsList3\n",
    "dfLZC1 = pd.DataFrame(subResults1, ssfnames)\n",
    "dfLZC2 = pd.DataFrame(subResults2, ssfnames)\n",
    "dfLZC3 = pd.DataFrame(subResults3, ssfnames)\n",
    "\n",
    "\n",
    "# save\n",
    "dfLZC1.to_csv(f'./results/NewDataResults/LZC/0dot2SecEpochs/dfLZC.csv', index=True)  \n",
    "dfLZC2.to_csv(f'./results/NewDataResults/LZC/1SecEpochs/dfLZC.csv', index=True)  \n",
    "dfLZC3.to_csv(f'./results/NewDataResults/LZC/{window}/dfLZC.csv', index=True)  \n",
    "print ('   - dfLZC.csv is saved')\n",
    "display(dfLZC1)\n",
    "# save/write dict\n",
    "import json\n",
    "with open(f'./results/NewDataResults/LZC/0dot2SecEpochs/dict_0dot2SecEpochs.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList1, file)\n",
    "with open(f'./results/NewDataResults/LZC/1SecEpochs/dict_1SecEpochs.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList2, file)\n",
    "with open(f'./results/NewDataResults/LZC/5SecEpochs/dict_5SecEpochs.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList3, file)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUINCY BANDS----TenSecEpochs\n",
    "            # -Dif Frequincy bands \n",
    "            # -from derivative project 03 - cyan\n",
    "print ('FREQUINCY_BANDS_TenSecEpochs')\n",
    "\n",
    "####################\n",
    "#NEW DATA  AND FREQUENCY\n",
    "#MAIN                                                #(Complete)\n",
    "#window = 'TenSecEpochs_max' #'TenSecEpochs' #'TenSecEpochs_max_woutreref'#'TenSecEpochs_mat'#'TenSecEpochs_MatandMax' #'TenSecEpochs_woutreref' # window = '10secEpochs'\n",
    "#create save folders\n",
    "for  window in ['TenSecEpochs_delta','TenSecEpochs_theta','TenSecEpochs_alpha','TenSecEpochs_beta','TenSecEpochs_gamma']:\n",
    "    create_folder = f'./results/NewDataResults/LZC/{window}'\n",
    "    if not os.path.exists(create_folder):\n",
    "        os.makedirs(create_folder)\n",
    "        print (f'main folder created: {create_folder}')\n",
    "    else:\n",
    "        print(\"main save folder already exists.\")\n",
    "## 10secEpochs / Broadband\n",
    "fnames = sorted(os.listdir('./cutdata'))\n",
    "ssfnames = [f for f in fnames if 'HC05_IT1st' not in f]\n",
    "for f in ssfnames:\n",
    "    with io.capture_output() as captured:# Load data\n",
    "        raw = mne.io.read_raw_fif(f'./cutdata/{f}/{f}.fif', preload=True)\n",
    "        raw.info['bads']= []\n",
    "        raw2 = raw.copy().pick_types(eeg = True)\n",
    "        raw3 = nk.mne_crop(raw2.copy(),smin=0, smax=75000, include_tmax=False)\n",
    "    with io.capture_output() as captured:# Preprocessing\n",
    "        raw3.notch_filter(60)# Filter\n",
    "        raw3.filter(0.1, 50) #downsample after filtering??????\n",
    "        raw3.drop_channels(new_non_brain_el)# Remove non-brain channels\n",
    "        delta = raw3.copy().filter(1, 4)#Delta waves (0.5–4 Hz)\n",
    "        theta = raw3.copy().filter(4, 8)#Theta waves (4–8 Hz)\n",
    "        alpha = raw3.copy().filter(8, 12)#Alpha waves (8–13 Hz)\n",
    "        beta = raw3.copy().filter(12, 30)#Beta waves (13–35 Hz)\n",
    "        gamma = raw3.copy().filter(30, 40)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Re-refrence\n",
    "        delta.set_eeg_reference(ref_channels='average')\n",
    "        theta.set_eeg_reference(ref_channels='average')\n",
    "        alpha.set_eeg_reference(ref_channels='average')\n",
    "        beta.set_eeg_reference(ref_channels='average')\n",
    "        gamma.set_eeg_reference(ref_channels='average')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #vars()['raw_'+f] = raw3.copy() # Saving variable names \n",
    "        #data = raw3.copy()# Epoch\n",
    "        epochs1 = mne.make_fixed_length_epochs(delta.copy(), duration=10, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs1=epochs1.get_data() #(30, 91, 2500)\n",
    "        epochs2 = mne.make_fixed_length_epochs(theta.copy(), duration=10, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs2=epochs2.get_data() #(30, 91, 2500)\n",
    "        epochs3 = mne.make_fixed_length_epochs(alpha.copy(), duration=10, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs3=epochs3.get_data() #(30, 91, 2500)\n",
    "        epochs4 = mne.make_fixed_length_epochs(beta.copy(), duration=10, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs4=epochs4.get_data() #(30, 91, 2500)\n",
    "        epochs5 = mne.make_fixed_length_epochs(gamma.copy(), duration=10, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs5=epochs5.get_data() #(30, 91, 2500)\n",
    "    vars()[f] = Aepochs1\n",
    "    vars()[f+'_1SecEpochs'] = Aepochs2\n",
    "    vars()[f+'_5SecEpochs'] = Aepochs3\n",
    "    vars()[f+'_TenSecEpochs_beta'] = Aepochs4\n",
    "    vars()[f+'_TenSecEpochs_gamma'] = Aepochs5\n",
    "    \n",
    "# main dataframe\n",
    "subResults1 = [] # Results from each subjects AverageLZc (average across participant's segments)\n",
    "subResults2 = []\n",
    "subResults3 = []\n",
    "subResults4 = []\n",
    "subResults5 = []\n",
    "dict_LZcSegmentsList1 = {} # LZc values for each segment (NOT averaged)\n",
    "dict_LZcSegmentsList2 = {}\n",
    "dict_LZcSegmentsList3 = {}\n",
    "dict_LZcSegmentsList4 = {}\n",
    "dict_LZcSegmentsList5 = {} \n",
    "#for  window in ['0dot2SecEpochs','1SecEpochs','5SecEpochs']:\n",
    "for f in ssfnames:\n",
    "    data1 = vars()[f]\n",
    "    data2 = vars()[f+'_1SecEpochs']\n",
    "    data3 = vars()[f+'_5SecEpochs']\n",
    "    data4 = vars()[f+'_TenSecEpochs_beta']\n",
    "    data5 = vars()[f+'_TenSecEpochs_gamma']\n",
    "    segments1, channels1, timepoints1 = shape(data1) #(30, 91, 2500)\n",
    "    segments2, channels2, timepoints2 = shape(data2) #(30, 91, 2500)\n",
    "    segments3, channels3, timepoints3 = shape(data3) #(30, 91, 2500)\n",
    "    segments4, channels4, timepoints4 = shape(data4) #(30, 91, 2500)\n",
    "    segments5, channels5, timepoints5 = shape(data5) #(30, 91, 2500)\n",
    "    LZcSegmentsList1 = []\n",
    "    LZcSegmentsList2 = []\n",
    "    LZcSegmentsList3 = []\n",
    "    LZcSegmentsList4 = []\n",
    "    LZcSegmentsList5 = []\n",
    "    for i in range(segments1):\n",
    "        X = data1[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList1.append(LZcSegment)\n",
    "        #print (f'{i} --- {LZcSegment}')\n",
    "    for i in range(segments2):\n",
    "        X = data2[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList2.append(LZcSegment)\n",
    "    for i in range(segments3):\n",
    "        X = data3[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList3.append(LZcSegment)\n",
    "    for i in range(segments4):\n",
    "        X = data4[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList4.append(LZcSegment)\n",
    "    for i in range(segments5):\n",
    "        X = data5[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList5.append(LZcSegment)\n",
    "    AverageLZc1 = sum(LZcSegmentsList1)/len(LZcSegmentsList1) #mean(LZcSegmentsList)\n",
    "    AverageLZc2 = sum(LZcSegmentsList2)/len(LZcSegmentsList2) #mean(LZcSegmentsList)\n",
    "    AverageLZc3 = sum(LZcSegmentsList3)/len(LZcSegmentsList3) #mean(LZcSegmentsList)\n",
    "    AverageLZc4 = sum(LZcSegmentsList4)/len(LZcSegmentsList4) #mean(LZcSegmentsList)\n",
    "    AverageLZc5 = sum(LZcSegmentsList5)/len(LZcSegmentsList5) #mean(LZcSegmentsList)\n",
    "    print ('_____',f, '_____', AverageLZc1)\n",
    "    vars()['AverageLZC_'+f] = AverageLZc1\n",
    "    subResults1.append(AverageLZc1)\n",
    "    subResults2.append(AverageLZc2)\n",
    "    subResults3.append(AverageLZc3)\n",
    "    subResults4.append(AverageLZc4)\n",
    "    subResults5.append(AverageLZc5)\n",
    "    dict_LZcSegmentsList1[f] = LZcSegmentsList1\n",
    "    dict_LZcSegmentsList2[f] = LZcSegmentsList2\n",
    "    dict_LZcSegmentsList3[f] = LZcSegmentsList3\n",
    "    dict_LZcSegmentsList4[f] = LZcSegmentsList4\n",
    "    dict_LZcSegmentsList5[f] = LZcSegmentsList5\n",
    "dfLZC1 = pd.DataFrame(subResults1, ssfnames)\n",
    "dfLZC2 = pd.DataFrame(subResults2, ssfnames)\n",
    "dfLZC3 = pd.DataFrame(subResults3, ssfnames)\n",
    "dfLZC4 = pd.DataFrame(subResults4, ssfnames)\n",
    "dfLZC5 = pd.DataFrame(subResults5, ssfnames)\n",
    "\n",
    "# save\n",
    "dfLZC1.to_csv(f'./results/NewDataResults/LZC/TenSecEpochs_delta/dfLZC.csv', index=True)  \n",
    "dfLZC2.to_csv(f'./results/NewDataResults/LZC/TenSecEpochs_theta/dfLZC.csv', index=True)  \n",
    "dfLZC3.to_csv(f'./results/NewDataResults/LZC/TenSecEpochs_alpha/dfLZC.csv', index=True)  \n",
    "dfLZC4.to_csv(f'./results/NewDataResults/LZC/TenSecEpochs_beta/dfLZC.csv', index=True)  \n",
    "dfLZC5.to_csv(f'./results/NewDataResults/LZC/TenSecEpochs_gamma/dfLZC.csv', index=True) \n",
    "print ('   - dfLZC.csv is saved')\n",
    "display(dfLZC1)\n",
    "# save/write dict\n",
    "import json\n",
    "with open(f'./results/NewDataResults/LZC/TenSecEpochs_delta/dict_TenSecEpochs_delta.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList1, file)\n",
    "with open(f'./results/NewDataResults/LZC/TenSecEpochs_theta/dict_TenSecEpochs_theta.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList2, file)\n",
    "with open(f'./results/NewDataResults/LZC/TenSecEpochs_alpha/dict_TenSecEpochs_alpha.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList3, file)\n",
    "with open(f'./results/NewDataResults/LZC/TenSecEpochs_beta/dict_TenSecEpochs_beta.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList4, file)\n",
    "with open(f'./results/NewDataResults/LZC/TenSecEpochs_gamma/dict_TenSecEpochs_gamma.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList5, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad654df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUINCY BANDS----0dot02SecEpochs\n",
    "            # -Dif Frequincy bands \n",
    "            # -from derivative project 03 - cyan\n",
    "print ('FREQUINCY_BANDS for 0.02-SecEpochs')\n",
    "\n",
    "####################\n",
    "#NEW DATA  AND FREQUENCY\n",
    "#MAIN                                                #(Complete)\n",
    "#window = 'TenSecEpochs_max' #'TenSecEpochs' #'TenSecEpochs_max_woutreref'#'TenSecEpochs_mat'#'TenSecEpochs_MatandMax' #'TenSecEpochs_woutreref' # window = '10secEpochs'\n",
    "#create save folders\n",
    "for  window in ['0dot02SecEpochs_delta','0dot02SecEpochs_theta','0dot02SecEpochs_alpha','0dot02SecEpochs_beta','0dot02SecEpochs_gamma']:\n",
    "    create_folder = f'./results/NewDataResults/LZC/{window}'\n",
    "    if not os.path.exists(create_folder):\n",
    "        os.makedirs(create_folder)\n",
    "        print (f'main folder created: {create_folder}')\n",
    "    else:\n",
    "        print(\"main save folder already exists.\")\n",
    "## 10secEpochs / Broadband\n",
    "fnames = sorted(os.listdir('./cutdata'))\n",
    "ssfnames = [f for f in fnames if 'HC05_IT1st' not in f]\n",
    "for f in ssfnames:\n",
    "    with io.capture_output() as captured:# Load data\n",
    "        raw = mne.io.read_raw_fif(f'./cutdata/{f}/{f}.fif', preload=True)\n",
    "        raw.info['bads']= []\n",
    "        raw2 = raw.copy().pick_types(eeg = True)\n",
    "        raw3 = nk.mne_crop(raw2.copy(),smin=0, smax=75000, include_tmax=False)\n",
    "    with io.capture_output() as captured:# Preprocessing\n",
    "        raw3.notch_filter(60)# Filter\n",
    "        raw3.filter(0.1, 50) #downsample after filtering??????\n",
    "        raw3.drop_channels(new_non_brain_el)# Remove non-brain channels\n",
    "        delta = raw3.copy().filter(1, 4)\n",
    "        theta = raw3.copy().filter(4, 8)\n",
    "        alpha = raw3.copy().filter(8, 12)\n",
    "        beta = raw3.copy().filter(12, 30)\n",
    "        gamma = raw3.copy().filter(30, 40)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Re-refrence\n",
    "        delta.set_eeg_reference(ref_channels='average')\n",
    "        theta.set_eeg_reference(ref_channels='average')\n",
    "        alpha.set_eeg_reference(ref_channels='average')\n",
    "        beta.set_eeg_reference(ref_channels='average')\n",
    "        gamma.set_eeg_reference(ref_channels='average')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #vars()['raw_'+f] = raw3.copy() # Saving variable names \n",
    "        #data = raw3.copy()# Epoch\n",
    "        epochs1 = mne.make_fixed_length_epochs(delta.copy(), duration=0.02, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs1=epochs1.get_data() #(30, 91, 2500)\n",
    "        epochs2 = mne.make_fixed_length_epochs(theta.copy(), duration=0.02, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs2=epochs2.get_data() #(30, 91, 2500)\n",
    "        epochs3 = mne.make_fixed_length_epochs(alpha.copy(), duration=0.02, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs3=epochs3.get_data() #(30, 91, 2500)\n",
    "        epochs4 = mne.make_fixed_length_epochs(beta.copy(), duration=0.02, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs4=epochs4.get_data() #(30, 91, 2500)\n",
    "        epochs5 = mne.make_fixed_length_epochs(gamma.copy(), duration=0.02, reject_by_annotation=True, overlap=0.0)#epochs #number of epochs for participants should have a normal distribution\n",
    "        Aepochs5=epochs5.get_data() #(30, 91, 2500)\n",
    "    vars()[f] = Aepochs1\n",
    "    vars()[f+'_1SecEpochs'] = Aepochs2\n",
    "    vars()[f+'_5SecEpochs'] = Aepochs3\n",
    "    vars()[f+'_TenSecEpochs_beta'] = Aepochs4\n",
    "    vars()[f+'_TenSecEpochs_gamma'] = Aepochs5\n",
    "    \n",
    "# main dataframe\n",
    "subResults1 = [] # Results from each subjects AverageLZc (average across participant's segments)\n",
    "subResults2 = []\n",
    "subResults3 = []\n",
    "subResults4 = []\n",
    "subResults5 = []\n",
    "dict_LZcSegmentsList1 = {} # LZc values for each segment (NOT averaged)\n",
    "dict_LZcSegmentsList2 = {}\n",
    "dict_LZcSegmentsList3 = {}\n",
    "dict_LZcSegmentsList4 = {}\n",
    "dict_LZcSegmentsList5 = {} \n",
    "#for  window in ['0dot2SecEpochs','1SecEpochs','5SecEpochs']:\n",
    "for f in ssfnames:\n",
    "    data1 = vars()[f]\n",
    "    data2 = vars()[f+'_1SecEpochs']\n",
    "    data3 = vars()[f+'_5SecEpochs']\n",
    "    data4 = vars()[f+'_TenSecEpochs_beta']\n",
    "    data5 = vars()[f+'_TenSecEpochs_gamma']\n",
    "    segments1, channels1, timepoints1 = shape(data1) #(30, 91, 2500)\n",
    "    segments2, channels2, timepoints2 = shape(data2) #(30, 91, 2500)\n",
    "    segments3, channels3, timepoints3 = shape(data3) #(30, 91, 2500)\n",
    "    segments4, channels4, timepoints4 = shape(data4) #(30, 91, 2500)\n",
    "    segments5, channels5, timepoints5 = shape(data5) #(30, 91, 2500)\n",
    "    LZcSegmentsList1 = []\n",
    "    LZcSegmentsList2 = []\n",
    "    LZcSegmentsList3 = []\n",
    "    LZcSegmentsList4 = []\n",
    "    LZcSegmentsList5 = []\n",
    "    for i in range(segments1):\n",
    "        X = data1[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList1.append(LZcSegment)\n",
    "        #print (f'{i} --- {LZcSegment}')\n",
    "    for i in range(segments2):\n",
    "        X = data2[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList2.append(LZcSegment)\n",
    "    for i in range(segments3):\n",
    "        X = data3[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList3.append(LZcSegment)\n",
    "    for i in range(segments4):\n",
    "        X = data4[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList4.append(LZcSegment)\n",
    "    for i in range(segments5):\n",
    "        X = data5[i] \n",
    "        LZcSegment = LZc(X)\n",
    "        LZcSegmentsList5.append(LZcSegment)\n",
    "    AverageLZc1 = sum(LZcSegmentsList1)/len(LZcSegmentsList1) #mean(LZcSegmentsList)\n",
    "    AverageLZc2 = sum(LZcSegmentsList2)/len(LZcSegmentsList2) #mean(LZcSegmentsList)\n",
    "    AverageLZc3 = sum(LZcSegmentsList3)/len(LZcSegmentsList3) #mean(LZcSegmentsList)\n",
    "    AverageLZc4 = sum(LZcSegmentsList4)/len(LZcSegmentsList4) #mean(LZcSegmentsList)\n",
    "    AverageLZc5 = sum(LZcSegmentsList5)/len(LZcSegmentsList5) #mean(LZcSegmentsList)\n",
    "    print ('_____',f, '_____', AverageLZc1)\n",
    "    vars()['AverageLZC_'+f] = AverageLZc1\n",
    "    subResults1.append(AverageLZc1)\n",
    "    subResults2.append(AverageLZc2)\n",
    "    subResults3.append(AverageLZc3)\n",
    "    subResults4.append(AverageLZc4)\n",
    "    subResults5.append(AverageLZc5)\n",
    "    dict_LZcSegmentsList1[f] = LZcSegmentsList1\n",
    "    dict_LZcSegmentsList2[f] = LZcSegmentsList2\n",
    "    dict_LZcSegmentsList3[f] = LZcSegmentsList3\n",
    "    dict_LZcSegmentsList4[f] = LZcSegmentsList4\n",
    "    dict_LZcSegmentsList5[f] = LZcSegmentsList5\n",
    "dfLZC1 = pd.DataFrame(subResults1, ssfnames)\n",
    "dfLZC2 = pd.DataFrame(subResults2, ssfnames)\n",
    "dfLZC3 = pd.DataFrame(subResults3, ssfnames)\n",
    "dfLZC4 = pd.DataFrame(subResults4, ssfnames)\n",
    "dfLZC5 = pd.DataFrame(subResults5, ssfnames)\n",
    "\n",
    "# save\n",
    "dfLZC1.to_csv(f'./results/NewDataResults/LZC/0dot02SecEpochs_delta/dfLZC.csv', index=True)  \n",
    "dfLZC2.to_csv(f'./results/NewDataResults/LZC/0dot02SecEpochs_theta/dfLZC.csv', index=True)  \n",
    "dfLZC3.to_csv(f'./results/NewDataResults/LZC/0dot02SecEpochs_alpha/dfLZC.csv', index=True)  \n",
    "dfLZC4.to_csv(f'./results/NewDataResults/LZC/0dot02SecEpochs_beta/dfLZC.csv', index=True)  \n",
    "dfLZC5.to_csv(f'./results/NewDataResults/LZC/0dot02SecEpochs_gamma/dfLZC.csv', index=True) \n",
    "print ('   - dfLZC.csv is saved')\n",
    "display(dfLZC1)\n",
    "# save/write dict\n",
    "import json\n",
    "with open(f'./results/NewDataResults/LZC/0dot02SecEpochs_delta/dict_0dot02SecEpochs_delta.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList1, file)\n",
    "with open(f'./results/NewDataResults/LZC/0dot02SecEpochs_theta/dict_0dot02SecEpochs_theta.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList2, file)\n",
    "with open(f'./results/NewDataResults/LZC/0dot02SecEpochs_alpha/dict_0dot02SecEpochs_alpha.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList3, file)\n",
    "with open(f'./results/NewDataResults/LZC/0dot02SecEpochs_beta/dict_0dot02SecEpochs_beta.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList4, file)\n",
    "with open(f'./results/NewDataResults/LZC/0dot02SecEpochs_gamma/dict_0dot02SecEpochs_gamma.json', 'w') as file:\n",
    "    json.dump(dict_LZcSegmentsList5, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e710de7d",
   "metadata": {},
   "source": [
    "### Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading LZC values\n",
    "window = 'TenSecEpochs' # 'TenSecEpochs_max' # window = '10secEpochs'\n",
    "vars()[window]= pd.read_csv(f'./results/NewDataResults/LZC/{window}/dfLZC.csv')\n",
    "vars()[window].rename(columns={vars()[window].columns[0]: 'ID'}, inplace=True)\n",
    "vars()[window].rename(columns={vars()[window].columns[1]: 'LZC'}, inplace=True)\n",
    "vars()[window].set_index('ID', inplace=True)\n",
    "df = vars()[window].copy()\n",
    "display(df)\n",
    "\n",
    "\n",
    "# load/read dict\n",
    "import json\n",
    "with open(f'./results/NewDataResults/LZC/{window}/dict_{window}.json', 'r') as file:\n",
    "    loaded_dict = json.load(file)\n",
    "segsdf = pd.DataFrame(loaded_dict)\n",
    "display(segsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10824552",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175eadc0",
   "metadata": {},
   "source": [
    "# TimeSeries Feature Extraction-updated (complete)\n",
    "# Oct.11,2023\n",
    "\n",
    "Added dif time window options for feature extraction\n",
    "- 10_sec_windows\n",
    "- 5_sec_windows\n",
    "- 2_sec_windows\n",
    "\n",
    "\n",
    "for 10['svd1', 'svd2', 'approx', 'sample', 'detrended', 'katz', 'cwpen', 'bubben', 'll', 'mswpen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597109bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done Done \n",
    "#oct.11,2023=================================================================================================#\n",
    "\n",
    "# Loading and Preprocessing Data\n",
    "fnames = sorted(os.listdir('./cutdata'))\n",
    "for f in fnames:\n",
    "    with io.capture_output() as captured:# Load data\n",
    "        raw = mne.io.read_raw_fif(f'./cutdata/{f}/{f}.fif', preload=True)\n",
    "        raw.info['bads'] = []# Keep EEG only (will remove bads too; so I reset bads first)\n",
    "        raw2 = raw.copy().pick_types(eeg = True)# Crop from 312sec to 302seconds\n",
    "        raw3 = nk.mne_crop(raw2.copy(),smin=0, smax=75500, include_tmax=False)\n",
    "    with io.capture_output() as captured:# Preprocessing\n",
    "        raw3.notch_filter(60)# Filter\n",
    "        raw3.filter(0.1, 50)\n",
    "        raw3.drop_channels(new_non_brain_el)# Remove non-brain channels\n",
    "        raw3.set_eeg_reference(ref_channels='average')# Re-refrence\n",
    "    vars()[f] = raw3.copy()# Saving variable names \n",
    "\n",
    "###########################################\n",
    "for window in ['5secwindows']:#['2secwindows','5secwindows']: #['2secwindows','5secwindows','10secwindows']: ##<<<<<------------------------set the folder name\n",
    "###########################################\n",
    "    # create save folders\n",
    "    print (\"\\n=====================================================================\")\n",
    "    print(window)\n",
    "    print (\"=====================================================================\")\n",
    "    create_folder = f'./results/NewDataResults/save/{window}'\n",
    "    if not os.path.exists(create_folder):\n",
    "        os.makedirs(create_folder)\n",
    "        print (f'main folder created: {create_folder}')\n",
    "    else:\n",
    "        print(\"main save folder already exists.\")\n",
    "    # Participant group & Complexity measure\n",
    "    for m in ['approx', 'sample', 'detrended', 'katz', 'cwpen', 'bubben', 'll', 'mswpen']:#['svd1', 'svd2', 'approx', 'sample', 'detrended', 'katz', 'cwpen', 'bubben', 'll', 'mswpen']: #['svd1', 'svd2', 'approx']:#, 'sample', 'detrended', 'katz', 'cwpen', 'bubben', 'll', 'mswpen']:\n",
    "        print ('\\n************************',m,'************************\\n')\n",
    "        measure = m\n",
    "        # create numpy save folder\n",
    "        create_folder2 = f'./results/NewDataResults/save/{window}/{m}'\n",
    "        if not os.path.exists(create_folder2):\n",
    "            os.makedirs(create_folder2)\n",
    "            print (f'secondary folder created: {create_folder2}')\n",
    "        else:\n",
    "            print(f\"secondary folder already exists ({m}).\")\n",
    "\n",
    "        if m == 'svd1':\n",
    "            svd = lambda x: nk.entropy_svd(x, delay=1, dimension=2, show=False)[0]\n",
    "        elif m == 'svd2':\n",
    "            svd = lambda x: nk.entropy_svd(x, delay=4, dimension=30, show=False)[0]\n",
    "        elif m == 'approx':\n",
    "            svd = lambda x: ant.app_entropy(x)\n",
    "        elif m == 'sample':\n",
    "            svd = lambda x: ant.sample_entropy(x)\n",
    "        elif m == 'detrended':\n",
    "            svd = lambda x: ant.detrended_fluctuation(x)\n",
    "        elif m == 'katz':\n",
    "            svd = lambda x: nk.fractal_katz(x)[0]\n",
    "        elif m == 'cwpen':\n",
    "            svd = lambda x: nk.entropy_permutation(x, delay=1, dimension=3, corrected=True, weighted=True, conditional=True)[0]\n",
    "        elif m == 'bubben':\n",
    "            svd = lambda x: nk.entropy_bubble(x, delay=1, dimension=3, alpha=2)[0]   \n",
    "        elif m == 'll':\n",
    "            svd = lambda x: nk.fractal_linelength(x)[0]\n",
    "        elif m == 'mswpen':\n",
    "            svd = lambda x: nk.entropy_multiscale(x, scale='default', dimension=3, tolerance='sd', method=\"MSWPEn\", show=False)[0]        \n",
    "\n",
    "\n",
    "\n",
    "        # main dataframe\n",
    "        vars()[m+window]= pd.DataFrame()\n",
    "        for f in fnames:\n",
    "            print ('_____',f, '_____')\n",
    "            data = vars()[f]\n",
    "            Elist = []\n",
    "            if window == '10secwindows':\n",
    "                # 10secwindows (30 columns)\n",
    "                for e in data.ch_names:\n",
    "                    print (f'{f}_{e}', end=', ')\n",
    "                    signal = data.get_data(picks = e)[0][500:] #for avg channels --> #signal = data.pick_channels(e).get_data().mean(axis = 0)#[500:]\n",
    "                    signal_2d = signal.reshape(-1, 2500) #signal_2d = signal.reshape(-1, 1250)\n",
    "                    svd_2d = np.apply_along_axis(svd, axis=1, arr=signal_2d)\n",
    "                    Elist.append(svd_2d)\n",
    "            if window == '5secwindows':\n",
    "                # 5secwindows (60 columns)\n",
    "                for e in data.ch_names:\n",
    "                    print (f'{f}_{e}', end=', ')\n",
    "                    signal = data.get_data(picks = e)[0][500:]\n",
    "                    signal_2d = signal.reshape(-1, 1250)\n",
    "                    svd_2d = np.apply_along_axis(svd, axis=1, arr=signal_2d)\n",
    "                    Elist.append(svd_2d)\n",
    "            if window == '2secwindows':\n",
    "                # 2secwindows (151 columns)\n",
    "                for e in data.ch_names:\n",
    "                    print (f'{f}_{e}', end=', ')\n",
    "                    signal = data.get_data(picks = e)[0]\n",
    "                    signal_2d = signal.reshape(-1, 500)\n",
    "                    svd_2d = np.apply_along_axis(svd, axis=1, arr=signal_2d)\n",
    "                    Elist.append(svd_2d)\n",
    "            #subject-specific dataframe\n",
    "            vars()['df_'+f] = pd.DataFrame(Elist)\n",
    "            ID_with_suffix = [f+'_'+ID for ID in data.ch_names]\n",
    "            vars()['df_'+f].index = [ID_with_suffix]\n",
    "            #MAIN\n",
    "            vars()[m+window] = pd.concat([vars()[m+window],vars()['df_'+f]],axis=0) \n",
    "\n",
    "\n",
    "            # save\n",
    "            vars()['df_'+f].to_csv(f'./results/NewDataResults/save/{window}/{m}/df_{f}.csv', index=True) \n",
    "            print (f'   - df_{f}.csv is saved')\n",
    "        vars()[m+window].to_csv(f'./results/NewDataResults/save/{window}/{m}/{m}_{window}.csv', index=True)\n",
    "        print (f'________________________________________________________\\n   - saved The Big GUy!!!!! ({m}_{window}.csv)\\n________________________________________________________\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aeb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowed complexity values\n",
    "window = '10secwindows'\n",
    "for i in ['svd1', 'svd2', 'approx', 'sample', 'detrended', 'katz', 'cwpen', 'bubben', 'll', 'mswpen']:#['approx']:['ll','mswpen','bubben','cwpen']:#['svd2','svd1','approx','sample','detrended','katz']\n",
    "    vars()[i+'_'+window]= pd.read_csv(f'./results/NewDataResults/save/{window}/{i}/{i}_{window}.csv')\n",
    "    vars()[i+'_'+window].rename(columns={vars()[i+'_'+window].columns[0]: 'ID'}, inplace=True)\n",
    "    vars()[i+'_'+window].set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98343a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svd1_10secwindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac17d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e6ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af0f0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11419358",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
